{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca6476a-2daf-4622-b292-2c6cb0b01fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacaf9e5-76b7-45ae-8e5c-9bb2595295db",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA = Path(\"/Users/dmatekenya/LEARNING-LOCAL/coursera-pytorch/data/ESC-50-master/audio/\")\n",
    "aud_files = [i for i in DIR_DATA.iterdir() if i.suffix == \".wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417af117-76e7-44f9-bbf0-b749c8a7fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESC50(Dataset):\n",
    "\n",
    "    def __init__(self,path):\n",
    "        # Get directory listing from path\n",
    "        files = list(path.iterdir())\n",
    "        # Iterate through the listing and create a list of tuples (filename, label)\n",
    "        self.items = [(f, int(re.findall(r'\\d+', f.parts[-1].split(\"-\")[-1])[0])) for f in files]\n",
    "        self.length = len(self.items)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename, label = self.items[index]\n",
    "        audio_tensor, sample_rate = torchaudio.load(filename)\n",
    "        return audio_tensor, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6f8025-1a64-481a-bbc3-591042c5c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_esc50 = ESC50(DIR_DATA.joinpath(\"train\"))\n",
    "tensor, label = list(test_esc50)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cbb3f1e-630a-41f3-acf1-e7b765144bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "train_esc50 = ESC50(DIR_DATA.joinpath(\"train\"))\n",
    "valid_esc50 = ESC50(DIR_DATA.joinpath(\"valid\"))\n",
    "test_esc50  = ESC50(DIR_DATA.joinpath(\"test\"))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_esc50, batch_size = bs,\n",
    "                shuffle = True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_esc50, batch_size = bs,\n",
    "                shuffle = True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_esc50, batch_size = bs,\n",
    "                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9c8e73-215e-4e8b-b21f-0eded43545e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(256, 512, 3)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.avgPool = nn.AvgPool1d(30)\n",
    "        self.fc1 = nn.Linear(512, 10)\n",
    "        #self.bn_fc1 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.avgPool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        #x = self.bn_fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb0d4c4-dd3f-41cb-b2d7-15c04a507157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioNet(\n",
       "  (conv1): Conv1d(1, 128, kernel_size=(80,), stride=(4,))\n",
       "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
       "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(128, 256, kernel_size=(3,), stride=(1,))\n",
       "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
       "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (avgPool): AvgPool1d(kernel_size=(30,), stride=(30,), padding=(0,))\n",
       "  (fc1): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "audio_net = AudioNet()\n",
    "audio_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7cfb0d-378e-49d1-a7a8-b225f1adf741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_lr(model, loss_fn, optimizer, init_value=1e-8, final_value=10.0):\n",
    "    number_in_epoch = len(train_loader) - 1\n",
    "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    best_loss = 0.0\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for x, y in train_loader:\n",
    "        batch_num += 1\n",
    "        # inputs, labels = data\n",
    "        # inputs, labels = inputs, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        print(outputs.shape, \"====>\", y.shape)\n",
    "        loss = loss_fn(outputs, y)\n",
    "\n",
    "        # Crash out if loss explodes\n",
    "\n",
    "        if batch_num > 1 and loss > 4 * best_loss:\n",
    "            return log_lrs[10:-5], losses[10:-5]\n",
    "\n",
    "        # Record the best loss\n",
    "\n",
    "        if loss < best_loss or batch_num == 1:\n",
    "            best_loss = loss\n",
    "\n",
    "        # Store the values\n",
    "\n",
    "        losses.append(loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "\n",
    "        # Do the backward pass and optimize\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the lr for the next step and store\n",
    "\n",
    "        lr *= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "    return log_lrs[10:-5], losses[10:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dcfb522-b837-4381-a01b-e94b54a3c599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 7, 10]) ====> torch.Size([64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [64, 10], got [64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w3/dfvgjkh10wz8t573m_c20xf00000gp/T/ipykernel_93070/3017340154.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/w3/dfvgjkh10wz8t573m_c20xf00000gp/T/ipykernel_93070/424977082.py\u001b[0m in \u001b[0;36mfind_lr\u001b[0;34m(model, loss_fn, optimizer, init_value, final_value)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"====>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Crash out if loss explodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1165\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [64, 10], got [64]"
     ]
    }
   ],
   "source": [
    "torch.save(audio_net.state_dict(), \"audionet.pth\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(audio_net.parameters(), lr=0.001)\n",
    "logs,losses = find_lr(audio_net, criterion, optimizer)\n",
    "plt.plot(logs,losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c1cbbdf-b866-4b68-8086-de703f1360a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.Tensor([[ 0.1578,  0.0440, -0.1067,  0.2767,  0.3650, -0.2182,  0.3496,  0.2335,\n",
    "          0.0498,  0.1919],\n",
    "        [-0.0070,  0.0271, -0.0268,  0.2868,  0.3516, -0.1395,  0.3821,  0.3145,\n",
    "          0.1229,  0.1458],\n",
    "        [-0.3674, -0.0540,  0.1793,  0.2377,  0.3492, -0.1011,  0.3829,  0.4043,\n",
    "          0.3496,  0.1117],\n",
    "        [-0.2983, -0.0893,  0.2048,  0.1758,  0.3875, -0.1026,  0.4330,  0.3774,\n",
    "          0.3390,  0.1602],\n",
    "        [ 0.0198,  0.0008,  0.0238,  0.2398,  0.3958, -0.2059,  0.3762,  0.2661,\n",
    "          0.1393,  0.1569],\n",
    "        [ 0.1468,  0.0350, -0.0970,  0.2642,  0.3462, -0.2147,  0.3657,  0.2377,\n",
    "          0.0698,  0.1592],\n",
    "        [ 0.1005,  0.0045, -0.0455,  0.2572,  0.3750, -0.2092,  0.4024,  0.2566,\n",
    "          0.1059,  0.1475]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b908e62-1960-4498-a2d3-578c719bad03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a54e4cbc-fbcd-45f6-8209-61f11f4c2f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([12,  0, 49,  8, 14, 43, 41,  5, 34, 35, 16, 29, 13, 10, 33,  9, 45,  9,\n",
    "        23, 42, 40, 10, 39,  0, 49, 10,  7,  6, 16, 35, 16, 36,  3, 34,  7,  4,\n",
    "        25, 30, 36, 15, 40, 24, 17, 12, 21, 49,  9, 33, 12,  0, 26, 41, 38, 15,\n",
    "        14, 30, 48, 22, 17, 29, 28, 36, 19, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "670eedef-63f5-475a-a911-8a49563238bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = x.unsqueeze(1)\n",
    "x.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50c72b-139c-421f-87b1-4fe09eefbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "model.load(\"audionet.pth\")\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(audionet.parameters(), lr=lr)‚Äù\n",
    "\n",
    "Excerpt From\n",
    "Programming PyTorch for Deep Learning\n",
    "Ian Pointer\n",
    "This material may be protected by copyright."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
