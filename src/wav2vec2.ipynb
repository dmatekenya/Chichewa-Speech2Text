{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae2ecc01-cb40-4c23-84c6-b5081c791aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import random\n",
    "from IPython.display import display, HTML\n",
    "import IPython.display as ipd\n",
    "import re\n",
    "import shutil\n",
    "from sphfile import SPHFile\n",
    "import json\n",
    "from transformers import Wav2Vec2FeatureExtractor,  Wav2Vec2CTCTokenizer, Wav2Vec2Processor, Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e1cc27d-f200-4454-9319-fb915b05e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = Path.cwd().parents[0]\n",
    "DIR_DATA = CWD.joinpath('data', 'TIMIT')\n",
    "DIR_TRAIN = DIR_DATA.joinpath('TRAIN')\n",
    "DIR_TEST = DIR_DATA.joinpath('TEST')\n",
    "FILE_METADATA_TRAIN = DIR_DATA.joinpath('train_metadata.csv')\n",
    "FILE_METADATA_TEST = DIR_DATA.joinpath('test_metadata.csv')\n",
    "GENERATE_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7525109a-3bb1-4486-9d43-848e7fcf48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 28\n",
    "torch.set_num_threads(num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94e7594a-3ac5-4fe9-a8fb-1d157327c267",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_strange_audio_format_to_wav(ind):\n",
    "    \"\"\"\n",
    "    Convert the audios from some strange non-wav to wav format.\n",
    "    \"\"\"\n",
    "    for file in ind.iterdir():\n",
    "        if file.suffix == \".WAV\" or file.suffix == \".wav\":\n",
    "            fname = file.parts[-1].split(\".\")[0]\n",
    "            outf = ind.joinpath(\"{}_wav.wav\".format(fname))\n",
    "            sph = SPHFile(str(file))\n",
    "            sph.write_wav(filename = str(outf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f15929e7-b2c8-4e3f-a43f-24041d0512d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_files_to_wav():\n",
    "    \"\"\"\n",
    "    Get through the train and test folder to convert the files.\n",
    "    \"\"\"\n",
    "    # colect all folders into a list\n",
    "    folders_to_process = []\n",
    "    for folder in DIR_TEST.iterdir():\n",
    "        if folder.is_dir():\n",
    "            for next_folder in folder.iterdir():\n",
    "                if next_folder.is_dir():\n",
    "                    folders_to_process.append(next_folder)\n",
    "                    \n",
    "    \n",
    "    for folder in DIR_TRAIN.iterdir():\n",
    "        if folder.is_dir():\n",
    "            for next_folder in folder.iterdir():\n",
    "                if next_folder.is_dir():\n",
    "                    folders_to_process.append(next_folder)\n",
    "\n",
    "    # process the folders\n",
    "    test = random.choice(folders_to_process)\n",
    "    WAVS = [i for i in test.iterdir() if i.suffix == \".WAV\"]\n",
    "    assert len(WAVS) > 0 \n",
    "    for folder in folders_to_process:\n",
    "        convert_strange_audio_format_to_wav(ind=folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9f034a3-2603-48d5-9df3-afc3e4084574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_files_to_wav()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44fcc152-00d2-41c4-888e-636a542bac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_strange_audio_format_to_wav(ind=sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87446c33-474e-4079-8889-2ac82e017523",
   "metadata": {},
   "source": [
    "# Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6bf15-021f-4566-b4d5-8a90e06e14cb",
   "metadata": {},
   "source": [
    "## Define utility functions for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d3e7804-d5e6-4720-aeba-49a1cf61fc61",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_data_file(indir=None, output_file=None):\n",
    "    cnt = 1\n",
    "    data = []\n",
    "    for reg in indir.iterdir():\n",
    "        dia_reg = reg.parts[-1]\n",
    "        if reg.is_dir():\n",
    "            for user in reg.iterdir():\n",
    "                if user.is_dir():\n",
    "                    for file in user.iterdir():\n",
    "                        fname = file.parts[-1].split(\".\")[0]\n",
    "                        if \"wav\" in fname:\n",
    "                            cnt += 1\n",
    "                            TXT_file = user.joinpath(\"{}.TXT\".format(fname[:-4]))\n",
    "                            try:\n",
    "                                raw_txt = open(TXT_file, 'r').read()\n",
    "                                txt = \" \".join(raw_txt.split(\" \")[2:])\n",
    "                                txt = txt.replace(\"\\n\", \"\")\n",
    "                            except:\n",
    "                                print(raw_text)\n",
    "                                pass\n",
    "                            data_item = {'dialect_region': dia_reg, 'speaker_id': user.parts[-1],\n",
    "                                        'aud_file': file, 'trasnc_file': TXT_file, 'text': txt}\n",
    "                            data.append(data_item)\n",
    "    df = pd.DataFrame(data)\n",
    "    df['id'] = [i for i in range(1, len(df)+1)]\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b16b30e5-a84a-4986-82b2-950c2ede4ac8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_special_charecters(txt):\n",
    "    chars_to_ignore_regex = '[\\,\\),\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "    lower_txt = txt.lower()\n",
    "\n",
    "    txt2 = re.sub(chars_to_ignore_regex, '', lower_txt)\n",
    "    return txt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f80ef3d-1285-4e16-80cf-f1a261c2b614",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_files_to_wav(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            curr_fpath = Path(row['aud_file'])\n",
    "            fname = curr_fpath.parts[-1].split(\".\")[0]\n",
    "            new_fpath = curr_fpath.joinpath(curr_fpath.parents[0], \"{}_wav.wav\".format(fname))\n",
    "            sph = SPHFile(str(curr_fpath))\n",
    "            sph.write_wav(filename = str(new_fpath))\n",
    "            df.loc[idx, 'aud_file'] = str(new_fpath)\n",
    "        except:\n",
    "            print('Something went wrong')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d29ecb14-310b-4c55-bf15-1a1ced7adbf3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_weird_audio_format2wav():\n",
    "    df = convert_files_to_wav(df=df_test)\n",
    "    df.to_csv(FILE_METADATA_TEST, index=False)\n",
    "    \n",
    "    df = convert_files_to_wav(df=df_train)\n",
    "    df.to_csv(FILE_METADATA_TRAIN, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "703f722c-0cf8-4fc3-872d-56e6a74105de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_all_chars(df, col):\n",
    "    text_vals = list(df[col].values)\n",
    "    all_text = \" \".join(text_vals)\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": vocab, \"all_text\": all_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40f69bce-26ed-426d-bb0a-36006fc2f552",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_files_to_wav(ind):\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            curr_fpath = Path(row['aud_file'])\n",
    "            fname = curr_fpath.parts[-1].split(\".\")[0]\n",
    "            new_fpath = curr_fpath.joinpath(curr_fpath.parents[0], \"{}_wav.wav\".format(fname))\n",
    "            sph = SPHFile(str(curr_fpath))\n",
    "            sph.write_wav(filename = str(new_fpath))\n",
    "            df.loc[idx, 'aud_file'] = str(new_fpath)\n",
    "        except:\n",
    "            print('Something went wrong')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c9a5ad-dbee-4b62-bfca-1d41a3fe573a",
   "metadata": {},
   "source": [
    "## Process the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03725f30-3074-4d41-9122-2b858fdcee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATE_DATA:\n",
    "    df_test = generate_data_file(indir=DIR_TEST, output_file=FILE_METADATA_TEST)\n",
    "    df_train = generate_data_file(indir=DIR_TRAIN, output_file=FILE_METADATA_TRAIN)\n",
    "else:\n",
    "    df_test = pd.read_csv(FILE_METADATA_TEST)\n",
    "    df_train = pd.read_csv(FILE_METADATA_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4985de3a-2228-4b2d-9781-9cb1e2ad4467",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_random_elements(df, num_examples=10):\n",
    "    assert num_examples <= len(df), \"Can't pick more elements than there are in the dataset.\"\n",
    "    indices = df.index.values\n",
    "    picks = random.choices(indices, k=num_examples)\n",
    "    df = df.iloc[picks][['text']]\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "706945ba-5aa5-4bac-a918-534121fe4927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>When all else fails, use force.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Don't ask me to carry an oily rag like that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>She had your dark suit in greasy wash water all year.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>To honor him is to honor ourselves.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3402</th>\n",
       "      <td>She had your dark suit in greasy wash water all year.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>For a moment, boy and mount hung in midair.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4121</th>\n",
       "      <td>Of course, dear, she said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>She saw me and sat down beside me, three feet away.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>His talk turns to what he calls the mess, or sometimes this buzzing confusion.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>They used an aggressive policeman to flag thoughtless motorists.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(df=df_train, num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84aa9d0c-1f61-4101-9d64-373a0d2564c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"text_no_quotes\"] = df_train.text.apply(lambda x: remove_special_charecters(x))\n",
    "df_test[\"text_no_quotes\"] = df_test.text.apply(lambda x: remove_special_charecters(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa51f99f-2dd4-4575-a22f-39e87b041881",
   "metadata": {},
   "source": [
    "# Generate Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54009c04-41ca-4f2a-a642-f0f7447c3e7b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_vocubalary(df_train, df_test, output_vocab_file):\n",
    "    res_train = extract_all_chars(df_train, \"text_no_quotes\")\n",
    "    res_test = extract_all_chars(df_test, \"text_no_quotes\")\n",
    "    \n",
    "    vocab_list = list(set(res_train[\"vocab\"]) | set(res_test[\"vocab\"]))\n",
    "    \n",
    "    vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "    vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "    del vocab_dict[\" \"]\n",
    "    \n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "    \n",
    "\n",
    "    with open(vocab_file, 'w') as file:\n",
    "        json.dump(vocab_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d2e0ecf-485b-4a04-8118-bed1a14f159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = DIR_DATA.joinpath('vocab.json')\n",
    "generate_vocubalary(df_train, df_test, output_vocab_file=vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f902e3-d557-436c-bebc-b76e5c23e79d",
   "metadata": {},
   "source": [
    "# Prepare Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5109a684-c8b0-4b50-9e11-0684e94c008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Wav2Vec2CTCTokenizer(str(vocab_file), unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "PROCESSOR = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51cb7534-db96-472d-b626-d045d4c3f779",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(row):\n",
    "    sample_rate, audio = wavfile.read(row['aud_file'])\n",
    "    \n",
    "    if sample_rate != 16000:\n",
    "        sample_rate = 16000\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    input_val = PROCESSOR(np.asarray(audio), sampling_rate=sample_rate).input_values[0]\n",
    "    \n",
    "    with PROCESSOR.as_target_processor():\n",
    "        processed_txt = PROCESSOR(row[\"text\"]).input_ids\n",
    "    \n",
    "    return input_val, processed_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a613896-c1f6-4c9c-bd38-e6a859d63842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['input_values'] = df_train.apply(lambda x: prepare_dataset(x)[0], axis=1)\n",
    "df_train['labels'] = df_train.apply(lambda x: prepare_dataset(x)[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5895472-e7cb-45ce-a804-d1dd573158bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['input_values'] = df_test.apply(lambda x: prepare_dataset(x)[0], axis=1)\n",
    "df_test['labels'] = df_test.apply(lambda x: prepare_dataset(x)[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05aa2f8e-90de-46d1-88c7-241b5c785769",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['input_values', 'labels']\n",
    "dataset_train = Dataset.from_pandas(df_train[cols_to_keep])\n",
    "dataset_test = Dataset.from_pandas(df_test[cols_to_keep])\n",
    "ds = DatasetDict()\n",
    "\n",
    "ds['train'] = dataset_train\n",
    "ds['test'] = dataset_test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693be717-ff01-4b20-97d5-3ac47a4637cc",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85ce4d4c-481b-4387-ba67-0442bd355f05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c2cc642f-2f40-4a10-ac9a-21aefdfc8312",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=PROCESSOR, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "149ec735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59cb9fe9-38b3-42c5-95e8-b60670328f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wer_metric = load_metric(\"wer\")\n",
    "except ImportError:\n",
    "    !pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a477904-83c6-46ce-9ee8-a10418f6ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = PROCESSOR.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = PROCESSOR.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = PROCESSOR.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9358f3d-4317-4a1a-8396-ca879edbf894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/facebook/wav2vec2-base/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/c7746642f045322fd01afa31271dd490e677ea11999e68660a92619ec7c892b4.ce1f96bfaf3d7475cb8187b9668c7f19437ade45fb9ceb78d2b06a2cec198015\n",
      "/home/ubuntu/.local/lib/python3.6/site-packages/transformers/configuration_utils.py:359: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n",
      "Model config Wav2Vec2Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"freeze_feat_extract_train\": true,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"no_mask_channel_overlap\": false,\n",
      "  \"no_mask_time_overlap\": false,\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 29,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/wav2vec2-base/resolve/main/pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/transformers/ef45231897ce572a660ebc5a63d3702f1a6041c4c5fb78cbec330708531939b3.fcae05302a685f7904c551c8ea571e8bc2a2c4a1777ea81ad66e47f7883a650a\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['project_hid.weight', 'quantizer.codevectors', 'project_q.bias', 'project_hid.bias', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_q.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\", \n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=PROCESSOR.tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58951b2f-3437-4051-8367-94dcce796aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1677: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2de9920b-ed3c-44fd-9136-3aebd9d5d96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_output_dir = CWD.joinpath('models', 'eng_models')\n",
    "epochs = 5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=model_output_dir,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=epochs,\n",
    "  gradient_checkpointing=True, \n",
    "  save_steps=500,\n",
    "  eval_steps=500,\n",
    "  logging_steps=500,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1f631406-2e9b-40b7-b6db-e2b663053634",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    tokenizer=PROCESSOR.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a75666eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ebc745-801b-4078-af47-a35cf5834ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 4620\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1445\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='1445' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 153/1445 16:36 < 2:22:02, 0.15 it/s, Epoch 0.53/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "trainer.train()\n",
    "end = datetime.now()\n",
    "diff = (end - start).total_seconds()/60\n",
    "print('Training {} epochs took {} minutes using {} cores'.format(epochs, int(diff), num_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75daecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
